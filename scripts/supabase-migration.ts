import { createClient } from '@supabase/supabase-js';
import * as fs from 'fs';
import * as path from 'path';
import { parse } from 'csv-parse/sync';

const supabaseUrl = process.env.VITE_SUPABASE_URL!;
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY!;

if (!supabaseUrl || !supabaseServiceKey) {
  throw new Error('Missing Supabase credentials: VITE_SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY are required');
}

// Client Supabase avec service role key (droits admin)
const supabase = createClient(supabaseUrl, supabaseServiceKey, {
  auth: {
    autoRefreshToken: false,
    persistSession: false
  }
});

async function executeSQL(sql: string, description: string) {
  console.log(`\nüîÑ ${description}...`);
  
  const { data, error } = await supabase.rpc('exec_sql', { sql_query: sql });
  
  if (error) {
    console.error(`‚ùå Erreur: ${error.message}`);
    throw error;
  }
  
  console.log(`‚úÖ ${description} - OK`);
  return data;
}

async function createTables() {
  console.log('\nüì¶ √âTAPE 3: Cr√©ation des tables Supabase\n');
  console.log('='.repeat(50));
  
  const sqlFile = fs.readFileSync(
    path.join(process.cwd(), 'supabase_migration', '01_create_tables.sql'),
    'utf8'
  );
  
  // D√©couper le fichier SQL en plusieurs commandes
  const commands = sqlFile
    .split(';')
    .map(cmd => cmd.trim())
    .filter(cmd => cmd.length > 0 && !cmd.startsWith('--'));
  
  let successCount = 0;
  
  for (const command of commands) {
    try {
      // Utiliser l'API REST directement car rpc peut ne pas exister
      const { error } = await supabase.from('_migrations').select('*').limit(1);
      
      // Si pas d'erreur, la table existe, sinon on execute via une autre m√©thode
      // On va utiliser fetch directement
      const response = await fetch(`${supabaseUrl}/rest/v1/rpc/exec`, {
        method: 'POST',
        headers: {
          'apikey': supabaseServiceKey,
          'Authorization': `Bearer ${supabaseServiceKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ query: command + ';' })
      });
      
      if (!response.ok) {
        // Essayer avec l'approche alternative: cr√©er directement via SQL
        console.log(`‚ö†Ô∏è  Commande ignor√©e (probablement d√©j√† ex√©cut√©e)`);
      } else {
        successCount++;
      }
    } catch (err: any) {
      console.log(`‚ö†Ô∏è  ${err.message}`);
    }
  }
  
  console.log(`\n‚úÖ ${successCount} commandes SQL ex√©cut√©es avec succ√®s`);
}

async function createTablesDirectly() {
  console.log('\nüì¶ √âTAPE 3: Cr√©ation des tables Supabase (m√©thode directe)\n');
  console.log('='.repeat(50));
  
  // Cr√©er les tables une par une en utilisant l'API Supabase
  
  // Pour Supabase, on doit cr√©er les tables via le SQL Editor ou migrations
  // Comme on n'a pas acc√®s direct au SQL, on va cr√©er un fichier de migration
  
  console.log('üìù Les tables doivent √™tre cr√©√©es manuellement dans Supabase.');
  console.log('\nüìã Instructions:');
  console.log('1. Allez dans votre projet Supabase ‚Üí SQL Editor');
  console.log('2. Copiez le contenu de: supabase_migration/01_create_tables.sql');
  console.log('3. Ex√©cutez le script SQL complet');
  console.log('4. V√©rifiez que les 19 tables sont cr√©√©es');
  console.log('\n‚è≥ Appuyez sur Entr√©e une fois les tables cr√©√©es...');
}

async function importCSVData() {
  console.log('\nüì• √âTAPE 5: Import des donn√©es CSV ‚Üí Supabase\n');
  console.log('='.repeat(50));
  
  const csvDir = path.join(process.cwd(), 'neon_export_csv');
  const tables = fs.readdirSync(csvDir)
    .filter(file => file.endsWith('.csv'))
    .map(file => file.replace('.csv', ''));
  
  let totalImported = 0;
  let totalErrors = 0;
  
  for (const tableName of tables) {
    const csvPath = path.join(csvDir, `${tableName}.csv`);
    const csvContent = fs.readFileSync(csvPath, 'utf8');
    
    // Parser le CSV
    const records = parse(csvContent, {
      columns: true,
      skip_empty_lines: true,
      trim: true
    });
    
    if (records.length === 0) {
      console.log(`‚ö†Ô∏è  ${tableName}: table vide, skip`);
      continue;
    }
    
    console.log(`\nüìä Import ${tableName}: ${records.length} lignes`);
    
    // Convertir les user_id en UUID si n√©cessaire
    const cleanedRecords = records.map((record: any) => {
      const cleaned: any = {};
      
      for (const [key, value] of Object.entries(record)) {
        if (value === '' || value === null) {
          cleaned[key] = null;
        } else if (key.includes('user_id') || key === 'id' || key === 'requester_id' || key === 'recipient_id' || key === 'referred_by' || key === 'challenge_id' || key === 'season_id' || key === 'card_back_id' || key === 'related_id') {
          // V√©rifier si c'est un UUID valide
          const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$/i;
          if (value && !uuidRegex.test(value as string)) {
            console.log(`‚ö†Ô∏è  UUID invalide pour ${key}: ${value} - ignor√©`);
            cleaned[key] = null;
          } else {
            cleaned[key] = value;
          }
        } else if (key.includes('jsonb') || key === 'privacy_settings' || key === 'owned_avatars' || key === 'reward' || key === 'player_hand' || key === 'dealer_hand') {
          // Parser JSONB
          try {
            cleaned[key] = typeof value === 'string' ? JSON.parse(value as string) : value;
          } catch {
            cleaned[key] = value;
          }
        } else if (key.includes('_at') || key.includes('date')) {
          // Dates
          cleaned[key] = value || null;
        } else if (typeof value === 'string' && !isNaN(Number(value)) && (key.includes('coins') || key.includes('gems') || key.includes('xp') || key.includes('level') || key.includes('amount') || key.includes('cost'))) {
          // Nombres
          cleaned[key] = Number(value);
        } else if (value === 'true' || value === 'false') {
          cleaned[key] = value === 'true';
        } else {
          cleaned[key] = value;
        }
      }
      
      return cleaned;
    });
    
    // Ins√©rer par lots de 100
    const batchSize = 100;
    for (let i = 0; i < cleanedRecords.length; i += batchSize) {
      const batch = cleanedRecords.slice(i, i + batchSize);
      
      const { data, error } = await supabase
        .from(tableName)
        .insert(batch);
      
      if (error) {
        console.error(`  ‚ùå Erreur lot ${i}-${i + batch.length}: ${error.message}`);
        totalErrors += batch.length;
      } else {
        console.log(`  ‚úÖ Lot ${i}-${i + batch.length} import√©`);
        totalImported += batch.length;
      }
    }
  }
  
  console.log('\n' + '='.repeat(50));
  console.log(`\nüìä R√âSULTAT IMPORT:`);
  console.log(`   ‚úÖ Lignes import√©es: ${totalImported}`);
  console.log(`   ‚ùå Erreurs: ${totalErrors}`);
}

async function createTrigger() {
  console.log('\nüîß √âTAPE 4: Cr√©ation du trigger auth\n');
  console.log('='.repeat(50));
  
  const triggerSQL = `
-- Fonction pour cr√©er automatiquement le profil lors de l'inscription
CREATE OR REPLACE FUNCTION public.handle_new_user()
RETURNS trigger AS $$
BEGIN
  INSERT INTO public.users (id, username, email, coins, gems, tickets, created_at)
  VALUES (
    new.id,
    COALESCE(new.raw_user_meta_data->>'username', split_part(new.email, '@', 1)),
    new.email,
    5000,
    0,
    3,
    now()
  );
  RETURN new;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Trigger sur auth.users
DROP TRIGGER IF EXISTS on_auth_user_created ON auth.users;
CREATE TRIGGER on_auth_user_created
  AFTER INSERT ON auth.users
  FOR EACH ROW EXECUTE FUNCTION public.handle_new_user();
`;
  
  console.log('üìù Trigger SQL √† ex√©cuter dans Supabase SQL Editor:');
  console.log('\n' + triggerSQL);
  console.log('\nüìã Copiez ce SQL et ex√©cutez-le dans: Supabase ‚Üí SQL Editor');
  
  // Sauvegarder dans un fichier
  fs.writeFileSync(
    path.join(process.cwd(), 'supabase_migration', '02_create_trigger.sql'),
    triggerSQL
  );
  
  console.log('\n‚úÖ Fichier sauvegard√©: supabase_migration/02_create_trigger.sql');
}

async function main() {
  console.log('üöÄ MIGRATION NEON ‚Üí SUPABASE');
  console.log('='.repeat(50));
  console.log(`üìç Supabase URL: ${supabaseUrl}`);
  console.log('');
  
  try {
    // Tester la connexion
    const { data, error } = await supabase.from('_test').select('*').limit(1);
    console.log('‚úÖ Connexion Supabase √©tablie\n');
  } catch (err: any) {
    console.log('‚ö†Ô∏è  Note: Connexion Supabase OK (erreur normale si tables pas cr√©√©es)\n');
  }
  
  // √âtape 3: Instructions pour cr√©er les tables
  await createTablesDirectly();
  
  // Attendre confirmation utilisateur
  console.log('\n‚è≠Ô∏è  Pour continuer, l\'utilisateur doit:');
  console.log('   1. Cr√©er les tables via SQL Editor Supabase');
  console.log('   2. Relancer ce script pour import des donn√©es');
  console.log('\nüí° Commande: npx tsx scripts/supabase-migration-import.ts');
}

main().catch(console.error);
